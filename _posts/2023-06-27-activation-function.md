---
title: 激活函数
tags: [深度学习,数学]
article_header:
  type: cover
---
在深度学习领域中，神经网络是一种强大的工具，它可以模拟人脑神经元之间的相互作用，实现各种复杂的任务。而激活函数作为神经网络的重要组成部分，对于网络的性能和表征能力起着至关重要的作用。本文将探讨激活函数的重要性以及常用的几种激活函数。
<!--more-->
#### 1. 什么是激活函数
激活函数是神经网络中的一个关键组件，它对神经元的输入进行**非线性变换**，将其转化为输出信号。激活函数的主要作用是引入非线性特性，使神经网络能够更好地拟合非线性函数，从而提高网络的表达能力。
#### 2. 激活函数的重要性
如果神经网络中只使用线性激活函数，那么网络的表达能力将受到限制，无法处理复杂的非线性模式。激活函数的引入能够使网络具备更强的非线性表达能力，从而能够拟合更复杂的问题。
#### 3. 激活函数汇总
##### 3.1 Sigmoid
* 别名：Logistic 函数
* 表达式: $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
* 导数: $$\sigma'(x) = \sigma(x)(1-\sigma(x))$$
* 优点
  * 输出范围有限(0,1)
  * 连续函数，便于求导
* 缺点
  * 梯度消失
  * 输出非0均值
  * 指数形式，计算时间复杂度高

##### 3.2 Tanh
* 别名：双曲正切函数
* 表达式: $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
* 导数: $$f'(x) = 1-[f(x)]^2$$
* 优点:
  * 输出范围有限(-1,1)
  * 输出0均值
* 缺点:
  * 梯度消失
  * 指数形式，计算时间复杂度高


##### 3.3 ReLU

* 别名：整流线型单元，Rectified linear unit
* 表达式: $$f(x) = max(0, x)$$
* 优点:
  * 使用ReLU的SGD算法的收敛速度比 sigmoid 和 tanh 快。
  * 在x>0区域上，不会出现梯度饱和、梯度消失的问题。
  * 计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。
* 缺点:
  * ReLU的输出不是0均值的。
  * Dead ReLU Problem(神经元坏死现象)：ReLU在负数区域被kill的现象叫做dead relu。ReLU在训练的时很“脆弱”。在x<0时，梯度为0。
    * 产生这种现象的两个原因：参数初始化问题；learning rate太高导致在训练过程中参数更新太大。
  * 解决方法：采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

##### 3.4 Leaky ReLU

* 别名：渗漏线性单元
* 表达式: $$f(x) = max(0.01x, x)$$
* 优点:
  * 为了解决dead ReLU现象。用一个类似0.01的小值来初始化神经元，从而使得ReLU在负数区域更偏向于激活而不是死掉。

##### 3.5 PReLU

* 别名：参数整流线性单元
* 表达式: $$f(x) = max(\alpha x, x)$$
* 备注:
  * 其中$$\alpha$$不是固定的，是通过反向传播学习出来的。

##### 3.6 ELU

*   别名：指数线性单元，Exponential Linear Unit


$$f(n) =
\begin{cases}
x, & \text{if }\text{ is x > 0} \\
\alpha(exp(x) - 1), & \text{if }\text{ is x <= 0}
\end{cases}$$

* 优点:
  * 输出均值接近0
  * 没有Dead ReLU问题
* 缺点:
  * 指数计算复杂度

##### 3.7 GeLU

*   别名：高斯误差线性单元，Gaussian Error Linear Unit

##### 3.8 Maxout

Maxout是通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的边界，然后每一段取最大。

* 优点:
  * Maxout的拟合能力非常强，可以拟合任意的凸函数。
  * Maxout具有ReLU的所有优点，线性、不饱和性。
  * 不会出现神经元坏死的现象。
* 缺点:
  * 成K倍地增加了参数量。