---
title: 激活函数
tags: [深度学习,数学]
article_header:
  type: cover
---
在深度学习领域中，神经网络是一种强大的工具，它可以模拟人脑神经元之间的相互作用，实现各种复杂的任务。而激活函数作为神经网络的重要组成部分，对于网络的性能和表征能力起着至关重要的作用。本文将探讨激活函数的重要性以及常用的几种激活函数。
<!--more-->
## 1. 什么是激活函数
激活函数是神经网络中的一个关键组件，它对神经元的输入进行**非线性变换**，将其转化为输出信号。激活函数的主要作用是引入非线性特性，使神经网络能够更好地拟合非线性函数，从而提高网络的表达能力。
## 2. 激活函数的重要性
如果神经网络中只使用线性激活函数，那么网络的表达能力将受到限制，无法处理复杂的非线性模式。激活函数的引入能够使网络具备更强的非线性表达能力，从而能够拟合更复杂的问题。
## 3. 激活函数汇总
### 3.1 Sigmoid
<img src="/assets/images/articles/activation_function_sigmoid.png" alt="Sigmoid" width="500"/>

> Sigmoid函数最早由比利时数学家 Pierre François Verhulst 在1838年提出，他在试图通过数学模型描述人口增长的过程中，引入了一种S型曲线函数来表示群体增长的饱和特性，这个函数后来被称为Sigmoid函数。
20世纪50年代到60年代早期，随着感知机模型的发展，Sigmoid函数逐渐开始被应用于机器学习领域。这些早期的研究为后来的机器学习算法和神经网络的发展奠定了基础。
随着时间的推移，在神经网络不断变得更深的发展趋势下，sigmoid逐渐暴露出梯度消失问题，逐渐被淘汰。

常见的Sigmoid函数图为了凸显函数的特点，Y轴坐标刻度通常与X轴坐标刻度比例不一致。本文中采用一致坐标以展现函数更直观的样子。

**梯度消失问题**：Sigmoid函数的导数在输入较大或较小的情况下接近于零，导致在网络的反向传播过程中梯度逐渐减小，甚至可能趋近于零。这会导致深层网络的训练变得困难，因为较小的梯度会导致参数更新变得缓慢，使网络学习变得很慢。

**输出非零中心**：Sigmoid函数的输出范围在0到1之间，这导致神经元的输出总是正向的，不以零为中心。这可能导致在网络训练过程中的不对称性，影响网络的学习能力。

**计算开销问题**：Sigmoid函数的计算开销较大，因为它涉及指数运算。在深度神经网络中，大规模的指数计算会带来额外的计算负担。

基于以上几个原因Sigmoid函数在深度学习领域应用较少，仅在诸如逻辑回归模型等浅层网络中仍有应用。

* 别名：Logistic函数, S型函数, 乙状函数，费努瓦函数
* 表达式: $$\sigma(x) = \frac{1}{1 + e^{-x}}$$
* 导数: $$\sigma'(x) = \sigma(x)(1-\sigma(x))$$
* 优点
  * 输出范围有限(0,1)
  * 连续函数，便于求导
* 缺点
  * 梯度消失
  * 输出非0均值
  * 指数形式，计算时间复杂度高

### 3.2 Tanh
* 别名：双曲正切函数
* 表达式: $$f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$
* 导数: $$f'(x) = 1-[f(x)]^2$$
* 优点:
  * 输出范围有限(-1,1)
  * 输出0均值
* 缺点:
  * 梯度消失
  * 指数形式，计算时间复杂度高


### 3.3 ReLU

* 别名：整流线型单元，Rectified linear unit
* 表达式: $$f(x) = max(0, x)$$
* 优点:
  * 使用ReLU的SGD算法的收敛速度比 sigmoid 和 tanh 快。
  * 在x>0区域上，不会出现梯度饱和、梯度消失的问题。
  * 计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。
* 缺点:
  * ReLU的输出不是0均值的。
  * Dead ReLU Problem(神经元坏死现象)：ReLU在负数区域被kill的现象叫做dead relu。ReLU在训练的时很“脆弱”。在x<0时，梯度为0。
    * 产生这种现象的两个原因：参数初始化问题；learning rate太高导致在训练过程中参数更新太大。
  * 解决方法：采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。

### 3.4 Leaky ReLU

* 别名：渗漏线性单元
* 表达式: $$f(x) = max(0.01x, x)$$
* 优点:
  * 为了解决dead ReLU现象。用一个类似0.01的小值来初始化神经元，从而使得ReLU在负数区域更偏向于激活而不是死掉。

### 3.5 PReLU

* 别名：参数整流线性单元
* 表达式: $$f(x) = max(\alpha x, x)$$
* 备注:
  * 其中$$\alpha$$不是固定的，是通过反向传播学习出来的。

### 3.6 ELU

*   别名：指数线性单元，Exponential Linear Unit


$$f(n) =
\begin{cases}
x, & \text{if }\text{ is x > 0} \\
\alpha(exp(x) - 1), & \text{if }\text{ is x <= 0}
\end{cases}$$

* 优点:
  * 输出均值接近0
  * 没有Dead ReLU问题
* 缺点:
  * 指数计算复杂度

### 3.7 GeLU

*   别名：高斯误差线性单元，Gaussian Error Linear Unit

### 3.8 Maxout

Maxout是通过分段线性函数来拟合所有可能的凸函数来作为激活函数的，但是由于线性函数是可学习，所以实际上是可以学出来的激活函数。具体操作是对所有线性取最大，也就是把若干直线的交点作为分段的边界，然后每一段取最大。

* 优点:
  * Maxout的拟合能力非常强，可以拟合任意的凸函数。
  * Maxout具有ReLU的所有优点，线性、不饱和性。
  * 不会出现神经元坏死的现象。
* 缺点:
  * 成K倍地增加了参数量。